---
title: "An illustrative example of the BASIS guidelines in practice"
output:
  html_document:
    df_print: paged
  pdf_document: default
bibliography: library.bib
editor_options: 
  markdown: 
    wrap: 72
---

![The Bayesian Simulation Study (BASIS) G¿uidelines](BASIS.png) As a toy
example, this R notebook prov≥id÷es an illustrative example of how to
apply the BASIS guidelines in practice. Therefore, a mini-simulation
study which compares two Bayesian approaches to hypothesis testing in a
linear regression model with a single predictor is planned, coded,
executed, analyzed and reported using the BASIS guidelines.

# I - Planning cycle

## Methods

For this neutral comparison study, two Bayesian methods for testing the
statistical hypothesis $H_0:\beta_1=0$ against $H_1:\beta_1 \neq 0$
inside a simple linear regression model with a single predictor are
selected. In the latter, $\beta_1$ denotes the single regression
coefficient. We do not outline the relevant literature in detail here,
as the study is only a toy example, but in a full-sized simulation study
one would report on the available theory and literature to justify the
choice of methods.

The first method included in this illustrative example is the standard
Bayesian approach to decide between a hypothesis $H_0:\theta =\theta_0$
and its alternative $H_1:\theta \neq \theta_0$ for a fixed
$\theta_0 \in \Theta$ based on the Bayes factor and posterior
probabilities [@Robert2007]:

$$\frac{P_{\vartheta|Y}(H_0)}{P_{\vartheta|Y}(H_1)}=\underbrace{\frac{f(y|H_0)}{f(y|H_1)}}_{\text{BF}_{01}(y)}\cdot \frac{P_{\vartheta}(H_0)}{P_{\vartheta}(H_1)}$$
In the above, the Bayes factor $\text{BF}_{01}(y)$ is the ratio of
marginal likelihoods. Based on fixed prior probabilities
$P_{\vartheta}(H_0)$ and $P_{\vartheta}(H_1)$ the posterior
probabilities $P_{\vartheta|Y}(H_0)$ and $P_{\vartheta|Y}(H_1)$ can be
evaluated after observing the data $y\in \mathcal{Y}$.

As an alternative to posterior probabilties to decide between $H_0$ and
$H_1$ the Bayesian e-value of the Full Bayesian Significance Test (FBST)
is used as an alternative. Details about the e-value $\text{ev}(H_0)$
are provided in @Pereira2020a and
@Kelter2021BehaviorResearchMethodsFBSTPackage. For the purpose of this
illustrative example it suffices to know that the e-value is a Bayesian
alternative to the frequentist p-value, and a usual decision threshold
is given as $\text{ev}(H_0)<\alpha$ for small $\alpha$ (e.g.
$\alpha=0.05$) to reject $H_0$.

## Aims

The aims of the neutral comparison study are to evaluate the probability
to reject $H_0$ when indeed $\beta=0$ holds, that is, the false-positive
error probability based on posterior probability of $H_0$ and the
e-value against $H_0$. As there is no explicit type I error concept in
the Bayesian paradigm, this question escapes analytic treatment and a
simulation study is conducted. It is important to make clear what
constitutes a Bayesian type I error because this could be a potential
cause of debate.

## Research question / Estimands

The research question is defined as follows: Which method yields a
smaller type I error rate and how to different prior choices
(informative or diffuse) relate to the error rate of each method?

### Specify hypothesis

Based on prior research such as @Kelter2021 and
@Kelter2020BayesianPosteriorIndices, it is conjectured that the e-value
will yield a larger type I error rate than the decision based on
posterior probability. This is due to the measure-theoretic nature of
Bayes factors and e-values, see @Kelter2021ComputationalBrainAndBehavior
and @LyWagenmakersFBST2021.

## Performance measures / Specific analytic strategy

A Monte Carlo simulation is performed which repeatedly runs the two
competing methods in simulated data sets. The performance measure of
interest is the rate of falsely rejected null hypotheses. Data is
simulated under $H_0:\beta_1=0$ and $H_0$ is subsequently tested against
$H_1:\beta_1 \neq 0$ for each iteration. A type I error is defined as
$P_{\vartheta|Y}(H_0)<\alpha_p$ and $\text{ev}(H_0)<\alpha_e$ for
$\alpha_p=0.5$ and $\alpha_e=0.05$. Note that when
$P_{\vartheta|Y}(H_0)<0.5$ we would decide in favour of $H_0$
incorrectly when data are simulated under $H_0$. The threshold
$\alpha_e=0.05$ is based on the theory behind the e-value, which mimics
the frequentist p-value. The Monte Carlo estimate of the type I error
rate associated with the posterior probability method results in

$$\frac{1}{n_{\text{sim}}}\sum_{i=1}^{n_{\text{sim}}} 1_{P_{\vartheta|Y}(H_0)<0.5}(Y_i)$$

For the e-value approach to test $H_0:\beta_1 =0$ the Monte Carlo
estimate of the type I error rate results in
$$\frac{1}{n_{\text{sim}}}\sum_{i=1}^{n_{\text{sim}}} 1_{\text{ev}(H_0)<0.05}(Y_i)$$
where $Y_i$ denotes a simulated dataset and $n_{\text{sim}}$ denotes the
Monte Carlo simulation size.

## Data-generating mechanisms

A simple linear regression model is employed with a single predictor The
performance of Bayes factors / Posterior model probabilities and
e-values has by now only been compared in two-sample test settings,
compare @Kelter2020BayesianPosteriorIndices. Data is generated according
to the simple linear regression model where for $y_i$, $i=1,...,n$, the
mean of the conditional distribution of $y_i$ given a $k\times 1$
predictor vector $\boldsymbol{\beta}_i$ is:
$$y_i=\boldsymbol{x}_i^T \boldsymbol{\beta} + \varepsilon_i$$ where
$\boldsymbol{\beta}$ is the $k\times 1$ vector of regression
coefficients and $$\varepsilon_i \sim \mathcal{N}(0,\sigma^2)$$ are the
independent and identically distributed residual errors. We assume that
based on the context, the predictor follows a standard normal
distribution, but the measurement error $\varepsilon_i$ is substantial
in comparison. We suppose that literature suggests $\sigma^2=5$.

### Likelihood

This yields the following likelihood function:
$$f(\boldsymbol{y}|\boldsymbol{X},\boldsymbol{\beta},\sigma^2)\propto (\sigma^2)^{-\frac{n}{2}}\exp \left ( -\frac{1}{2\sigma^2}(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})^{T}(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})\right )$$

### Priors

Priors must be chosen for $\boldsymbol{\beta}$ and $\sigma^2$. We use a
weakly-informative exponential prior for $\sigma^2$

$$\sigma^2 \sim \text{Exp}(\lambda)$$

by setting $\lambda=1$. Also, a weakly-informative normal prior
$$\beta_1 \sim \mathcal{N}(0,10)$$ is elicited for $\beta_1$.

We contrast this weakly-informative prior for $\beta_1$ with an
informative $\beta_1 \sim \mathcal{N}(0,1)$ prior on $\beta_1$. Our
sensitivity analyses will consist in comparing the results under these
two prior choices of interest.

We assume here, that the context of the simulation study justifies these
hyperparameters, compare the second cycle of the Bayesian research cycle
(BRC).

## Data simulation strategy

We assume $\sigma^2:=5$ based on the noise expected in the application
context, $\beta_1:=0$, that is, $H_0:\beta_1=0$ is true, and the value
of $\sigma^2$ is assumed to be justified due to the context of the
simulation study (second cycle of BRC). The specific data simulation
strategy thus consists in simulating $X_i\sim \mathcal{N}(0,1)$,
$\varepsilon_i \sim \mathcal{N}(0,1)$ for $i=1,...,n_{\text{obs}}$ and
to set
$\boldsymbol{Y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}=\boldsymbol{X}\beta_1+\boldsymbol{\varepsilon}$.
We assume that the application context renders $n_{\text{obs}}:=50$ the
data set size of interest. For example, this could amount to the sample
size which can be achieved in a trial.

## Intermediate considerations

Note that we have now finished the planning cycle, and also answered the
points listed in the preamble and Step 1 of the Bayesian analysis
reporting guidelines (BARG) [@Kruschke2021], compare the diagram of the
BASIS guidelines above. Also, we have provided information about what
sensitivity analyses we plan (Step 5 in the BARG, Point 8 and 9 in the
WAMBS). As no multivariate variance priors are used here, point 7 of the
WAMBS is obsolete.

# II - Coding and execution cycle

## Algorithm

We use Hamiltonian Monte Carlo sampling as the algorithm to obtain the
posterior. In particular, we employ the No-U-Turn sampler of
@Hoffman2014 as it does not require specification of hyperparameters for
the Hamiltonian Monte Carlo algorithm such as step length.

## Software

The R language and environment for statistical computing is used
[@RProgrammingLanguage]. The probabilistic programming language Stan
[@Gelman2015], version 2.18 is employed which implements the No-U-Turn
sampler [@Carpenter2017]. The R package `rstanarm` @Goodrich2020 version
2.21.1 is used for inference which builds on the package `rstan`
[@RStan2022], version 2.21.3.

Next to the software to obtain the posterior, the R software packages
`bayesplot` version 1.9.0 [@GabryBayesplot2022], `dplyr` version 1.0.8
[@Wickham2022] and `ggplot2` version 3.3.5 [@WickhamGGPlot2] are used to
visualize the results, perform prior and posterior predictive checks,
produce traceplots and process data when analyzing the results.

The computation of the posterior probability is possible solely based on
Gaussian kernel density estimation based on the posterior HMC samples as
outlined in the Methods section above. Gaussian kernel density
estimation is available in R [@Hothorn2014]. The computation of the
e-value is performed based on the posterior HMC samples via the R
package `fbst` [@Kelter2022fbstPackage], for details see
@Kelter2021BehaviorResearchMethodsFBSTPackage.

All seeds for data-generation and MCMC are specified in the accompanying
replication scripts, compare the reporting cycle.

## Convergence diagnostics

To monitor convergence to the posterior, 4 chains are used for
inference. We start with a chain length of $2000$ per chain and a warmup
of 1000 iterations. No thinning is applied and the total post-warmup
draws are thus 4000 draws. We check the resulting Monte Carlo standard
error and potential-scale-reduction-factor $\hat{R}$ to see whether
increasing the chain length is necessary. When the chain length suffices
based on the pilot simulations, we transition to the main simulations.
We only shift to the main simulations if the
potential-scale-reduction-factor $\hat{R}$ is smaller than $1.05$, and
tolerate a maximum Monte Carlo standard error per posterior simulation
of $0.05$ regarding $\beta_1$ and $\sigma^2$.

For the pilot simulations, we perform visual inspection of the
traceplots and prior and posterior predictive checks. In the main
simulations, we perform convergence diagnostics for each iteration based
on $\hat{R}$, the Monte Carlo standard error $<0.05$ for $\beta_1$ and
$\sigma^2$.

In the main simulations, we use the bootstrap-after-jackknife estimator
$\text{MCE}_{\text{JK}} (\hat{\theta}_{\text{MC}}(\boldsymbol{Y}))$ for
the Monte Carlo standard error of the performance measure of interest,
the type I error rate of each method. Therefore, we require less then
one percent standard error for the type I error rate for the Monte Carlo
estimates for each method, that is:
$\text{MCE}_{\text{JK}} (\hat{\theta}_{\text{MC}}(\boldsymbol{Y}))< 0.01$.

Note that Step 2 of the BARG is now answered for each
simulation-in-simulation of the comparison study.

## Pilot simulations

### Data-generation

```{r,include = FALSE}
set.seed(4200)
sigmaSquared = 5
beta1 = 0
n = 25
X = rnorm(n,0,1)
epsilon = rnorm(n,0,sigmaSquared)
Y = X*beta1+epsilon
sim_data = data.frame(Y=Y,X=X)
# head(sim_data)
# plot(sim_data)
```

### Prior predictive checks

```{r,message = FALSE,warning = FALSE,include=FALSE}
library(rstanarm)
options(mc.cores = parallel::detectCores())

prior_predictive_weakly_informative <- stan_glm(Y ~ X, data = NULL,
                  family = gaussian(link = "identity"),
                  prior = normal(location = 0, scale = 10, autoscale = FALSE),
                  prior_intercept = normal(location = 0, scale = 10, autoscale = FALSE),
                  prior_aux = exponential(1),
                  seed = 12345,
                  chains = 4,
                  iter = 2000,
                  warmup = 1000)
prior_summary(prior_predictive_weakly_informative)

prior_predictive_informative <- stan_glm(Y ~ X, data = NULL,
                  family = gaussian(link = "identity"),
                  prior = normal(location = 0, scale = 1, autoscale = FALSE),
                  prior_intercept = normal(location = 0, scale = 10, autoscale = FALSE),
                  prior_aux = exponential(1),
                  seed = 12345,
                  chains = 4,
                  iter = 2000,
                  warmup = 1000)
prior_summary(prior_predictive_informative)
```

Fit of the weakly informative prior predictive:

```{r,message = FALSE,warning = FALSE,fig.align = 'center',include = FALSE}
prior_predictive_weakly_informative$stanfit
```

```{r,message = FALSE,warning = FALSE,fig.align = 'center',include = FALSE}
prior_predictive_informative$stanfit
```

```{r, fig.show="hold", out.width="48%", echo = FALSE}
pp_check(prior_predictive_weakly_informative, nreps = 100)
pp_check(prior_predictive_informative, nreps = 100)
```

The prior predictive check looks reasonable in light of what we would
expect of the marginal density of $\boldsymbol{Y}$.

### Posterior inference and convergence diagnostics

```{r,message = FALSE,echo=FALSE,include = FALSE}
posterior_weakly_informative <- stan_glm(Y ~ X, data = sim_data,
                  family = gaussian(link = "identity"),
                  prior = normal(location = 0, scale = 10, autoscale = FALSE),
                  prior_intercept = normal(location = 0, scale = 10, autoscale = FALSE),
                  prior_aux = exponential(1),
                  seed = 12345,
                  chains = 4,
                  iter = 2000,
                  warmup = 1000)

posterior_informative <- stan_glm(Y ~ X, data = sim_data,
                  family = gaussian(link = "identity"),
                  prior = normal(location = 0, scale = 1, autoscale = FALSE),
                  prior_intercept = normal(location = 0, scale = 10, autoscale = FALSE),
                  prior_aux = exponential(1),
                  seed = 12345,
                  chains = 4,
                  iter = 2000,
                  warmup = 1000)
```

Posterior fit based on weakly-informative prior:

```{r,echo = FALSE}
posterior_weakly_informative$stanfit
# summary(posterior)
```

Posterior fit based on informative prior:

```{r,echo = FALSE}
posterior_informative$stanfit
# summary(posterior)
```

The $\hat{R}$ values are all below $1.05$, the effective sample size is
sufficient, which can be judged by the `se_mean` column which shows that
Monte Carlo standard errors for $\beta_1$ and $\sigma$ are less than or
equal to $0.05$, as required.

Next, we inspect traceplots of $\beta_1$ and $\sigma^2$:

```{r, fig.show="hold", out.width="48%",echo = FALSE}
par(mar = c(4, 4, 4, 4))
library(bayesplot)
color_scheme_set("mix-blue-pink")
p1 <- mcmc_trace(posterior_weakly_informative,  pars = c("X", "sigma"), n_warmup = 1000,
                facet_args = list(nrow = 2))
p1 + facet_text(size = 15)

p2 <- mcmc_trace(posterior_informative,  pars = c("X", "sigma"), n_warmup = 1000,
                facet_args = list(nrow = 2))
p2 + facet_text(size = 15)
```

The traceplots show no pathologies such as irregular jumps which would
indicate that chains have not reached their stationary distribution.
Rerunning the script and doubling the chain length as recommended in the
WAMBS point 3 results in the same conclusion.

We inspect whether chains suffer from strong autocorrelation via
autocorrelation plots:

```{r, fig.show="hold", out.width="48%",echo = FALSE}
par(mar = c(4, 4, 4, 4))
mcmc_acf(posterior_weakly_informative, pars = c("X", "sigma"))
mcmc_acf(posterior_informative, pars = c("X", "sigma"))
```

We can see that autocorrelation quickly vanishes for even small lag
values both under the weakly informative prior (left plot) and under the
informative prior (right plot).

Next, we inspect the histograms of the posterior draws under the weakly
informative prior:

```{r, fig.show="hold", out.width="48%",echo = FALSE}
mcmc_hist(posterior_weakly_informative, pars = c("X", "sigma"), binwidth=0.02)
library(bayesplot)
library(ggplot2)
plot_title <- ggtitle("Posterior distributions",
                      "with medians and 95% credible intervals")
mcmc_areas(posterior_weakly_informative,
           pars = c("X", "sigma"),
           prob = 0.95) + plot_title
```

These show sufficient information (see point 4 in the WAMBS). The
marginal posterior distributions of $\beta_1$ and $\sigma$ in the right
plot based on Gaussian kernel density estimation look reasonable (point
6 in the WAMBS, Step 3 in the BARG). For the informative prior, the
situation is similar:

```{r, fig.show="hold", out.width="48%",echo = FALSE}
mcmc_hist(posterior_informative, pars = c("X", "sigma"), binwidth=0.02)
library(bayesplot)
library(ggplot2)
plot_title <- ggtitle("Posterior distributions",
                      "with medians and 95% credible intervals")
mcmc_areas(posterior_informative,
           pars = c("X", "sigma"),
           prob = 0.95) + plot_title
```

### Posterior predictive checks

Next, we run posterior predictive checks for the weakly informative
prior fit:

```{r, fig.show="hold", out.width="48%",echo = FALSE}
par(mar = c(4, 4, 4, 4))
pp_check(posterior_weakly_informative, nreps = 100)
pp_check(posterior_weakly_informative, plotfun = "boxplot", nreps = 10, notch = FALSE)
```

These look reasonable, except for the fact that the marginal densities
have more mass in the area around zero. However, this is still in
agreement with the variability we expect the model to show regarding its
posterior predictions. For the posterior based on the informative prior,
the situation is likewise:

```{r, fig.show="hold", out.width="48%",echo = FALSE}
par(mar = c(4, 4, 4, 4))
pp_check(posterior_informative, nreps = 100)
pp_check(posterior_informative, plotfun = "boxplot", nreps = 10, notch = FALSE)
```

Note that we have iterated through points 2-6 of the WAMBS checklist
[@Depaoli2017] and Step 1e (prior predictive checks), Step 2 (describing
the posterior) and Step 3a (posterior predictive checks) of the BARG
[@Kruschke2021].

## Main simulations

For the main simulations, two methods must be implemented. The e-value
$\text{ev}(H_0)$ and the posterior probability $P_{\vartheta|Y}(H_0)$ of
$H_0:\beta_1=0$. To obtain the posterior probability of $H_0$, the prior
probabilities $P_{\vartheta}(H_0)=P_{\vartheta}(H_0)=0.5$ are used, so
that $\text{BF}_{01}(y)$ equals the posterior odds
$P_{\vartheta|Y}(H_0)/P_{\vartheta|Y}(H_1)$. Then,
$$P_{\vartheta|Y}(H_0)=\text{BF}_{01}(y)\cdot [1-P_{\vartheta|Y}(H_0)]=\text{BF}_{01}(y)-\text{BF}_{01}(y)\cdot P_{\vartheta|Y}(H_0)$$
so one obtains
$$P_{\vartheta|Y}(H_0)\cdot [1+\text{BF}_{01}(y)]=\text{BF}_{01}(y) \Leftrightarrow P_{\vartheta|Y}(H_0) = \frac{\text{BF}_{01}(y)}{1+\text{BF}_{01}(y)}$$
The Bayes factor $\text{BF}_{01}(y)$ is obtained using the
Savage-Dickey-density ratio
[@Dickey1970a,Verdinelli1995,Wagenmakers2010] as
$$\text{BF}_{01}(y)=\frac{p(\beta_1=0|y)}{p(\beta_1)}$$ where $p(\cdot)$
denotes the prior density for $\beta_1$ and $p(\cdot|y)$ the conditional
prior density given observed data $y$.

Computing the performance measure -- the type I error rate of each
method -- then reduces to simulating $n_{\text{sim}}$ data sets as
specified above, fitting the model and computing the e-value and Bayes
factor. The threshold $\text{ev}(H_0)<0.05$ is used to reject
$H_0:\beta_1=0$ based on $\text{ev}(H_0)$, and the threshold
$P_{\vartheta|Y}(H_0)<0.05$ is used to reject $H_0:\beta_1=0$ based on
the posterior probability $P_{\vartheta|Y}(H_0)$.

The sample size is varied from $n=10$ to $n=100$, and the main
simulations are repeated for the weakly informative (diffuse) prior
$\beta_1 \sim \mathcal{N}(0,10)$ and the informative prior
$\beta_1 \sim \mathcal{N}(0,1)$.

```{r,message=FALSE,echo=FALSE,include = FALSE}
start <- Sys.time()
set.seed(42)
sigmaSquared = 5
beta1 = 0
alpha_evalue = 0.05
alpha_postProb = 0.5
n = 25
nsim = 3000
library(fbst)
library(rstan)
rstan_options(auto_write = TRUE)
postProbH0_diffuse <- postProbH0_informative <- numeric(nsim) # to store the posterior probabilities of H0
evH0_diffuse <- evH0_informative <- numeric(nsim) # to store the e-values against H0
Rhat_beta1_diffuse <- Rhat_sigma_diffuse <- SE_beta1_diffuse <- SE_sigma_diffuse <- numeric(nsim)
Rhat_beta1_informative <- Rhat_sigma_informative <- SE_beta1_informative <- SE_sigma_informative <- numeric(nsim)
random_number_generator_seeds <- matrix(0,nrow=nsim,ncol=626) # stores the random number generator seeds
# Main simulation routine
for(i in 1:nsim){ 
  # Save the random number generator seed
  random_number_generator_seeds[i,] <- .Random.seed
  
  # Simulate data
  X = rnorm(n,0,1)
  epsilon = rnorm(n,0,sigmaSquared)
  Y = X*beta1+epsilon
  sim_data = data.frame(Y=Y,X=X)
  
  # Fit posteriors for weakly informative and informative priors
  posterior_diffuse <- stan_glm(Y ~ X, data = sim_data,
                  family = gaussian(link = "identity"),
                  prior = normal(location = 0, scale = 10, autoscale = FALSE),
                  prior_intercept = normal(location = 0, scale = 10, autoscale = FALSE),
                  prior_aux = exponential(1),
                  seed = 12345,
                  chains = 4,
                  iter = 2000,
                  warmup = 1000,
                  refresh = 0)
  
  posterior_informative <- stan_glm(Y ~ X, data = sim_data,
                  family = gaussian(link = "identity"),
                  prior = normal(location = 0, scale = 1, autoscale = FALSE),
                  prior_intercept = normal(location = 0, scale = 10, autoscale = FALSE),
                  prior_aux = exponential(1),
                  seed = 12345,
                  chains = 4,
                  iter = 2000,
                  warmup = 1000,
                  refresh = 0)
  
  # Extract Rhat of each iteration for beta1 and sigma
  monitorFit_diffuse = (monitor(posterior_diffuse$stanfit, se=TRUE, print = FALSE))
  monitorFit_informative = (monitor(posterior_informative$stanfit, se=TRUE, print = FALSE))
  
  Rhat_beta1_diffuse[i] = monitorFit_diffuse$Rhat[1]
  SE_beta1_diffuse[i] = monitorFit_diffuse$se_mean[1]
  Rhat_sigma_diffuse[i] = monitorFit_diffuse$Rhat[2]
  SE_sigma_diffuse[i] = monitorFit_diffuse$se_mean[2]
  Rhat_beta1_informative[i] = monitorFit_informative$Rhat[1]
  SE_beta1_informative[i] = monitorFit_informative$se_mean[1]
  Rhat_sigma_informative[i] = monitorFit_informative$Rhat[2]
  SE_sigma_informative[i] = monitorFit_informative$se_mean[2]
  
  # Extract posterior draws for beta_1 and construct posterior density based on Gaussian kernel density estimation
  draws_diffuse <- as.matrix(posterior_diffuse)[,2]
  draws_diffuse_Sorted = sort(draws_diffuse, decreasing = FALSE)
  postDens_beta1_diffuse = approxfun(density(draws_diffuse_Sorted),rule=2)
  draws_informative <- as.matrix(posterior_informative)[,2]
  draws_informative_Sorted = sort(draws_informative, decreasing = FALSE)
  postDens_beta1_informative = approxfun(density(draws_informative_Sorted),rule=2)
  
  # Compute Bayes factor in favour of H_0:\beta_1=0
  BF_01_diffuse = postDens_beta1_diffuse(0)/dnorm(0,0,10)
  postProbH0_diffuse[i] = BF_01_diffuse/(1+BF_01_diffuse)
  BF_01_informative = postDens_beta1_informative(0)/dnorm(0,0,1)
  postProbH0_informative[i] = BF_01_informative/(1+BF_01_informative)
  
  # Compute e-value ev(H_0) against H_0
  evH0_diffuse[i] = fbst(draws_diffuse,nullHypothesisValue = 0,dimensionTheta = 2,dimensionNullset = 1)$eValue
  evH0_informative[i] = fbst(draws_informative,nullHypothesisValue = 0,dimensionTheta = 2,dimensionNullset = 1)$eValue
}
end <- Sys.time()
start-end
```

# III - Analysis cycle

### Format data for analysis

```{r,include = FALSE}
results <- data.frame(postProbH0_diffuse = postProbH0_diffuse,
                      postProbH0_informative = postProbH0_informative,
                      evH0_diffuse = evH0_diffuse, evH0_informative = evH0_informative, 
                      Rhat_beta1_diffuse = Rhat_beta1_diffuse,  Rhat_beta1_informative = Rhat_beta1_informative,
                      Rhat_sigma_diffuse = Rhat_sigma_diffuse, Rhat_sigma_informative = Rhat_sigma_informative,
                      SE_beta1_diffuse = SE_beta1_diffuse, SE_beta1_informative = SE_beta1_informative,
                      SE_sigma_diffuse = SE_sigma_diffuse, SE_sigma_informative = SE_sigma_informative)
```

### Graphical exploration

First, we plot the resulting posterior probabilities and e-values under
the diffuse and informative prior:

```{r, fig.show="hold", out.width="48%", echo = FALSE}
par(mar = c(4, 4, 4, 4))
plot(results$postProbH0_diffuse,pch=18,ylab=expression(P[vartheta ~ "|" ~ Y] * (H[0])),xlab="Iteration",col="black",ylim=c(0,1))
points(results$postProbH0_informative,pch=19,ylab=expression(P[vartheta ~ "|" ~ Y] * (H[0])),xlab="Iteration",col="blue",ylim=c(0,1))
legend("bottomright", legend=c("diffuse prior", "informative prior"),
       col=c("black", "blue"),pch=c(18,19), cex=1)
plot(results$evH0_diffuse,pch=18,ylab=expression(ev(H[0])),xlab="Iteration",col="black",ylim=c(0,1))
points(results$evH0_informative,pch=19,ylab=expression(ev(H[0])),xlab="Iteration",col="blue",ylim=c(0,1))
legend("bottomright", legend=c("diffuse prior", "informative prior"),
       col=c("black", "blue"),pch=c(18,19), cex=1)
```

Next, we visualize the $\hat{R}$ values for $\beta_1$ and $\sigma_1$
along with the threshold $\hat{R}=1.05$:

```{r, fig.show="hold", out.width="48%",echo = FALSE}
par(mar = c(4, 4, 4, 4))
plot(results$Rhat_beta1_diffuse,pch=17,ylab=expression(hat(R)),xlab="Iteration",ylim=c(1,1.05),main="beta1")
abline(h=1.05,col="blue",lty=2)
points(results$Rhat_beta1_informative,pch=19,col="blue")
legend("topright", legend=c("diffuse prior", "informative prior"),
       col=c("black", "blue"),pch=c(17,19), cex=1)
plot(results$Rhat_sigma_diffuse,pch=17,ylab=expression(hat(R)),xlab="Iteration",ylim=c(1,1.05),main="sigma")
points(results$Rhat_sigma_informative,pch=19,col="blue")
abline(h=1.05,col="blue",lty=2)
legend("topright", legend=c("diffuse prior", "informative prior"),
       col=c("black", "blue"),pch=c(17,19), cex=1)
```

Clearly, all simulations yield small enough $\hat{R}\leq 1.05$. Next, we
plot the standard errors for $\beta_1$ and $\sigma_1$ with threshold
$SE\leq 0.05$:

```{r, fig.show="hold", out.width="48%", echo = FALSE}
par(mar = c(4, 4, 4, 4))
plot(results$SE_beta1_diffuse,pch=17,ylab=expression("SE(" ~ beta[1] ~ ")"),xlab="Iteration",ylim=c(0,0.05),main="beta1")
points(results$SE_beta1_informative,pch=19,ylab=expression("SE(" ~ beta[1] ~ ")"),xlab="Iteration",ylim=c(0,0.05),main="beta1",col="blue")
abline(h=0.05,col="blue",lty=2)
legend("topright", legend=c("diffuse prior", "informative prior"),
       col=c("black", "blue"),pch=c(17,19), cex=1)

plot(results$SE_sigma_diffuse,pch=17,ylab=expression("SE(" ~ sigma ~ ")"),xlab="Iteration",ylim=c(0,0.05),main="sigma")
abline(h=0.05,col="blue",lty=2)
points(results$SE_sigma_informative,pch=19,ylab=expression("SE(" ~ sigma ~ ")"),xlab="Iteration",ylim=c(0,0.05),main="sigma",col="blue")
abline(h=0.05,col="blue",lty=2)
legend("topright", legend=c("diffuse prior", "informative prior"),
       col=c("black", "blue"),pch=c(17,19), cex=1)
```

Like the $\hat{R}$ values, all simulations yield acceptable standard
errors for $\beta_1$ and $\sigma$.

## Estimands and Performance measures

Next, we report the Monte Carlo estimates of the type I error rates
associated with both methods. The type I error rate estimate for the
posterior probability based on the diffuse prior is given as

```{r,message=FALSE,echo=FALSE}
library(dplyr)
# diffuse prior results
typeIerrorRateBF_diffuse = (results %>% select(postProbH0_diffuse) %>% filter(postProbH0_diffuse < alpha_postProb) %>% count())$n/nsim
typeIerrorRateBF_diffuse
```

For the e-values the type I error rate Monte Carlo estimate under the
diffuse prior results in:

```{r,message=FALSE,echo=FALSE}
typeIerrorRateEValue_diffuse = (results %>% select(evH0_diffuse) %>% filter(evH0_diffuse < alpha_evalue) %>% count())$n/nsim
typeIerrorRateEValue_diffuse
```

Shifting to the informative prior, we obtain the following type I error
rate for the posterior probability based on the diffuse prior:

```{r,message=FALSE,echo=FALSE}
# informative prior results
typeIerrorRateBF_informative = (results %>% select(postProbH0_informative) %>% filter(postProbH0_informative < alpha_postProb) %>% count())$n/nsim
typeIerrorRateBF_informative
```

For the e-values the type I error rate Monte Carlo estimate under the
informative prior results in:

```{r,message=FALSE,echo=FALSE}
typeIerrorRateEValue_informative = (results %>% select(evH0_informative) %>% filter(evH0_informative < alpha_evalue) %>% count())$n/nsim
typeIerrorRateEValue_informative
```

### Compute jackknife-after-bootstrap Monte Carlo standard error estimation

Based on Appendix A.3 in the BASIS guidelines, we employ
jackknife-after-bootstrap resampling to compute the Monte Carlo standard
error for the performance measure of interest, that is, the type I error
rate of both methods. The simple Monte Carlo estimate for the type I
error rate is given as

$$\frac{1}{n_{\text{sim}}}\sum_{i=1}^{n_{\text{sim}}} 1_{P_{\vartheta|Y}(H_0)<0.5}(Y_i)$$
for the posterior probability and as
$$\frac{1}{n_{\text{sim}}}\sum_{i=1}^{n_{\text{sim}}} 1_{\text{ev}(H_0)<0.05}(Y_i)$$
for the type I error rate of the e-value. Instead of leaving out the
data set $Y_i$, we can equivalently leave out the $i$-th posterior
distribution, respectively the $i$-th indicator function
$1_{P_{\vartheta|Y}(H_0)<0.5}(Y_i)$ respectively
$1_{\text{ev}(H_0)<0.05}(Y_i)$. Then, the jacknife-after-bootstrap
estimate for the Monte Carlo standard error
$$\text{MCE}_{\text{JK}} (\hat{\theta}_{\text{MC}}(\boldsymbol{Y}))=\sqrt{\frac{n_{\text{sim}}-1}{n_{\text{sim}}}\sum_{i=1}^{n_{\text{sim}}}\left (\hat{\theta}_{\text{MC}}^{n_{\text{sim}}-1}(\boldsymbol{Y}_{-i})- \overline{\hat{\theta}_{\text{MC}}^{n_{\text{sim}}-1}(\boldsymbol{Y})}\right )^2}$$
is computed by first computing
$$\overline{\hat{\theta}_{\text{MC}}^{n_{\text{sim}}-1}(\boldsymbol{Y})}:=\frac{1}{n_{\text{sim}}}\sum_{i=1}^{n_{\text{sim}}}\hat{\theta}_{\text{MC}}^{n_{\text{sim}}-1}(\boldsymbol{Y}_{-i})$$
where $\hat{\theta}_{\text{MC}}^{n_{\text{sim}}-1}(\boldsymbol{Y}_{-i})$
is obtained as
$$\hat{\theta}_{\text{MC}}^{n_{\text{sim}}-1}(\boldsymbol{Y}_{-i})=\frac{1}{n_{\text{sim}}-1}\sum_{i=1}^{n_{\text{sim}}-1} 1_{P_{\vartheta|Y}(H_0)<0.5}(Y_i)$$
for the posterior probability approach and as
$$\hat{\theta}_{\text{MC}}^{n_{\text{sim}}-1}(\boldsymbol{Y}_{-i})=\frac{1}{n_{\text{sim}}-1}\sum_{i=1}^{n_{\text{sim}}-1} 1_{\text{ev}(H_0)<0.05}(Y_i)$$
for the e-value approach for testing $H_0:\beta_1=0$ against
$H_1:\beta_1 \neq 0$.

```{r,echo=FALSE}
MC_YMinusi_postProb_vec_diffuse <- MC_YMinusi_eValues_vec_diffuse <- numeric(nsim)
MC_YMinusi_postProb_vec_informative <- MC_YMinusi_eValues_vec_informative <- numeric(nsim)
for(i in 1:nsim){
  MC_YMinusi_postProb_diffuse = MC_YMinusi_postProb_informative = 0
  MC_YMinusi_eValues_diffuse = MC_YMinusi_eValues_informative = 0
  for(j in 1:nsim){
    if(j!=i){
      if(results$postProbH0_diffuse[j]<alpha_postProb){
        MC_YMinusi_postProb_diffuse = MC_YMinusi_postProb_diffuse + 1;
      }
      if(results$evH0_diffuse[j]<alpha_evalue){
        MC_YMinusi_eValues_diffuse = MC_YMinusi_eValues_diffuse + 1;
      }
      
      if(results$postProbH0_informative[j]<alpha_postProb){
        MC_YMinusi_postProb_informative = MC_YMinusi_postProb_informative + 1;
      }
      if(results$evH0_informative[j]<alpha_evalue){
        MC_YMinusi_eValues_informative = MC_YMinusi_eValues_informative + 1;
      }
    }
  }
  MC_YMinusi_postProb_vec_diffuse[i] = MC_YMinusi_postProb_diffuse/(nsim-1)
  MC_YMinusi_eValues_vec_diffuse[i] = MC_YMinusi_eValues_diffuse/(nsim-1)
  MC_YMinusi_postProb_vec_informative[i] = MC_YMinusi_postProb_informative/(nsim-1)
  MC_YMinusi_eValues_vec_informative[i] = MC_YMinusi_eValues_informative/(nsim-1)
}

bar_MC_YMinusi_postProb_diffuse = sum(MC_YMinusi_postProb_diffuse)/nsim
bar_MC_YMinusi_eValues_diffuse = sum(MC_YMinusi_eValues_diffuse)/nsim
bar_MC_YMinusi_postProb_informative = sum(MC_YMinusi_postProb_informative)/nsim
bar_MC_YMinusi_eValues_informative = sum(MC_YMinusi_eValues_informative)/nsim
```

The Monte Carlo standard error jackknife-after-bootstrap estimates for
the diffuse prior for the posterior probability approach results in

```{r, echo = FALSE}
MCE_JK_postProbH0_diffuse = sqrt(((nsim-1)/nsim)*sum((MC_YMinusi_postProb_vec_diffuse-bar_MC_YMinusi_postProb_diffuse)^2))#
MCE_JK_postProbH0_diffuse
```

for the e-value approach in

```{r, echo = FALSE}
MCE_JK_evH0_diffuse = sqrt(((nsim-1)/nsim)*sum((MC_YMinusi_eValues_vec_diffuse-bar_MC_YMinusi_eValues_diffuse)^2))
MCE_JK_evH0_diffuse
```

The Monte Carlo standard error jackknife-after-bootstrap estimates for
the informative prior for the posterior probability approach results in

```{r, echo = FALSE}
MCE_JK_postProbH0_informative = sqrt(((nsim-1)/nsim)*sum((MC_YMinusi_postProb_vec_informative-bar_MC_YMinusi_postProb_informative)^2))
MCE_JK_postProbH0_informative
```

and for the e-value approach in

```{r, echo = FALSE}
MCE_JK_evH0_informative = sqrt(((nsim-1)/nsim)*sum((MC_YMinusi_eValues_vec_informative-bar_MC_YMinusi_eValues_informative)^2))
MCE_JK_evH0_informative
```

## Prior sensitivity

Thus, visualizing the performance measures including the Monte Carlo
jackknife-after-bootstrap standard errors yields the following plot:

```{r, fig.show="hold", out.width="48%", echo = FALSE}
plot(c(1,1.2),c(typeIerrorRateBF_diffuse,typeIerrorRateEValue_diffuse),col="black",pch=18,main="Diffuse prior",xlab="Method",ylab="Type I error rate",xaxt="n",ylim=c(0,0.1),xlim=c(0.9,1.3))
axis(1, at=c(1,1.2), labels=c("Posterior probability","e-value"))
arrows(x0=1, y0=typeIerrorRateBF_diffuse-MCE_JK_postProbH0_diffuse, x1=1, y1=typeIerrorRateBF_diffuse+MCE_JK_postProbH0_diffuse, code=3, angle=90, length=0.1, col="black", lwd=2)
arrows(x0=1.2, y0=typeIerrorRateEValue_diffuse-MCE_JK_evH0_diffuse, x1=1.2, y1=typeIerrorRateEValue_diffuse+MCE_JK_evH0_diffuse, code=3, angle=90, length=0.1, col="black", lwd=2)

plot(c(1,1.2),c(typeIerrorRateBF_informative,typeIerrorRateEValue_informative),col="cornflowerblue",pch=18,main="Informative prior",xlab="Method",ylab="Type I error rate",xaxt="n",ylim=c(0,0.5),xlim=c(0.9,1.3))
axis(1, at=c(1,1.2), labels=c("Posterior probability","e-value"))
arrows(x0=1, y0=typeIerrorRateBF_informative-MCE_JK_postProbH0_informative, x1=1, y1=typeIerrorRateBF_informative+MCE_JK_postProbH0_informative, code=3, angle=90, length=0.1, col="cornflowerblue", lwd=2)
arrows(x0=1.2, y0=typeIerrorRateEValue_informative-MCE_JK_evH0_informative, x1=1.2, y1=typeIerrorRateEValue_informative+MCE_JK_evH0_informative, code=3, angle=90, length=0.1, col="cornflowerblue", lwd=2)
```

The resulting jacknife-after-bootstrap estimates of the Monte Carlo
standard errors associated with the type I error rates of the posterior
probability approach and the e-value approach result in
$\text{MCE}_{\text{JK}} (\hat{\theta}_{\text{MC}}(\boldsymbol{Y}))=0$
and
$\text{MCE}_{\text{JK}} (\hat{\theta}_{\text{MC}}(\boldsymbol{Y}))=0.02$
for the posterior probability and e-value.

# IV - Reporting cycle

## Reproducibility

Methods reproducibility is guaranteed as the planning cycle is outlined
in full detail in this notebook. All code and plots can be recreated by
executing the code hidden in this document, but available in the
.Rmd-file provided at the {r} [Open Science Framework]
(<https://osf.io/a9urj/>). There, a wiki is provided which outlines the
structure of the files. The codebook contains all seeds used for the
statistical simulations.

Note that we have specified a statistical analysis plan for the
performance measures in the planning cycle before performing any
simulations [@Held2020a] in the code and execution cycle, avoiding bias
and selective choice of performance measures which could be favourable
for one of the methods.

Results reproducibility is ensured by providing Monte Carlo standard
errors both on the simulation-in-simulation level (for $\beta_1$ and
$\sigma$ for each Monte Carlo iteration based on the one provided in
Stan, see @Geyer2011) as well as for the Monte Carlo estimate of the
associated type I error rate of each method, the performance measure of
interest in the study. We used bootstrap-after-jackknife resampling here
as advocated by @Koehler2009 and proposed by @Efron1992.

Inferential reproducibility is ensured through transparent reporting via
the BASIS, in particular the planning and analysis cycle. We stress that
all inferences are conditional upon the prior and model choices, the
selected sample size $n_{\text{obs}}=50$ as well as the subjective
definition of what constitutes a Bayesian type I error for the posterior
probability and e-value method. Different choices could lead to other
results, but we suppose that the former choices reflect the situation of
interest for the context at hand (e.g. a clinical trial analysis with a
specified sample size of $n_{\text{obs}}$ and the weakly versus
informative prior as the analysis and reference prior).

Also, we report the session information which includes all attached base
packages, R version, platform, and used operating system:

```{r,echo=FALSE}
sessionInfo()
```

## Publication and assessment

Results can be assessed by executing the corresponding R notebook
provided at the Open Science Foundation under {r} [Open Science
Framework] (<https://osf.io/a9urj/>). As the main simulations are
time-intensive, we provide a workspace image `workspace.RData` which
should be loaded first by executing the `load(workspace.RData)` command
in R after placing the workspace image `workspace.RData` into the
working directory of R.

## Reporting

We have arrived at the reporting of the results of the simulation study.
We conclude that based on the results, the type I error rate of the
posterior probability approach and e-values does not differ
systematically under the diffuse prior setting as indicated by the
overlapping Monte Carlo SE based confidence intervals and error rate of
$0.05$ in both cases.

In the informative prior setting, however, results show that the e-value
yields smaller type I error rate. We stress, however, that further prior
settings could be investigated, next to the influence of different
sample size on this result. Furthermore, the results observed hold only
under the assumption that a Bayesian type I error can be estimated based
on the thresholds given in the planning cycle in the performance measure
section.

We conclude that further simulations are required to investigate whether
the differences observed under informative priors in this simulation
study hold also for other sample sizes, prior settings and possibly
different thresholds for a Bayesian type I error.
